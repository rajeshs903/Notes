/*************** 88. Revision of Problem Statement and Design the solution ************start******* /

Problem Statement:

Use retail_db data set
Problem Statement
Get daily revenue by product considering completed and closed orders.
Data need to be sorted by ascending order by date and then descending order by revenue computed for each product for each day.
Data should be delimited by “,” in this order – order_date,daily_revenue_per_product,product_name
Data for orders and order_items is available in HDFS /public/retail_db/orders and /public/retail_db/order_items
Data for products are available locally under /data/retail_db/products
Final output need to be stored under
HDFS location – Avro format /user/YOUR_USER_ID/daily_revenue_avro_scala
HDFS location – text format /user/YOUR_USER_ID/daily_revenue_txt_scala
Local location /home/YOUR_USER_ID/daily_revenue_scala
The solution needs to be stored under /home/YOUR_USER_ID/daily_revenue_scala.txt

Solution:

Launch Spark Shell – Understand the environment and use resources optimally
Read orders and order_items
Filter for completed or closed orders
Convert both filtered orders and order_items to key-value pairs
Join the two data sets
Get daily revenue per product id
Load products from the local file system and convert into RDD
Join daily revenue per product id with products to get daily revenue per product (by name)
Sort the data by date in ascending order and by daily revenue per product in descending order
Get data to desired format – order_date,daily_revenue_per_product,product_name
Save final output into HDFS in Avro file format as well as text file format
HDFS location – Avro format /user/YOUR_USER_ID/daily_revenue_avro_scala
HDFS location – text format /user/YOUR_USER_ID/daily_revenue_txt_scala
Copy both from HDFS to the local file system
/home/YOUR_USER_ID/daily_revenue_scala


/*************** 88. Revision of Problem Statement and Design the solution ************end******* /


/*************** 89. Solution - Get Daily Revenue per Product - Launching Spark Shell ************start******* /

Solution – Get Daily Revenue per Product – Launching Spark Shell


Checking the size of the data

hadoop fs -ls -h /public/retail_db/orders
hadoop fs -ls -h /public/retail_db/order_items

// Launch Spark Shell - Understand the environment and use resources optimally
spark-shell --master yarn \
--num-executors 1 \
--executor-memory 512M \
--conf spark.ui.port=12673

/*************** 89. Solution - Get Daily Revenue per Product - Launching Spark Shell ************end*******/




/*************** 90. Solution - Get Daily Revenue per Product - Read and join orders and order_items ************start******* /

val orders=sc.textFile("/public/retail_db/orders")
val orderItems = sc.textFile("/public/retail_db/order_items")

orders.first
orderItems.first

orders.take(10).foreach(println)

1,2013-07-25 00:00:00.0,11599,CLOSED
2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT
3,2013-07-25 00:00:00.0,12111,COMPLETE
4,2013-07-25 00:00:00.0,8827,CLOSED
5,2013-07-25 00:00:00.0,11318,COMPLETE
6,2013-07-25 00:00:00.0,7130,COMPLETE
7,2013-07-25 00:00:00.0,4530,COMPLETE
8,2013-07-25 00:00:00.0,2911,PROCESSING
9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT
10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT

orderItems.take(10).foreach(println)

1,1,957,1,299.98,299.98
2,2,1073,1,199.99,199.99
3,2,502,5,250.0,50.0
4,2,403,1,129.99,129.99
5,4,897,2,49.98,24.99
6,4,365,5,299.95,59.99
7,4,502,3,150.0,50.0
8,4,1014,4,199.92,49.98
9,5,957,1,299.98,299.98
10,5,365,5,299.95,59.99

// Before filtering the order statuses COMPLETE,CLOSED,	first of all to know how many different order statuses exists in dataset
 orders.map(o=> o.split(",")(3)).distinct.collect.foreach(println)

PENDING_PAYMENT
CLOSED
CANCELED
PAYMENT_REVIEW
PENDING
ON_HOLD
PROCESSING
SUSPECTED_FRAUD
COMPLETE

//Dont use collect on large data output, it is a single thread RDD operation that may be drive in to memory issues.


val ordersFiltered = orders.filter(o => (o.split(",")(3) == "COMPLETE" ||o.split(",")(3) == "CLOSED"))


ordersFiltered.take(10).foreach(println)

1,2013-07-25 00:00:00.0,11599,CLOSED
3,2013-07-25 00:00:00.0,12111,COMPLETE
4,2013-07-25 00:00:00.0,8827,CLOSED
5,2013-07-25 00:00:00.0,11318,COMPLETE
6,2013-07-25 00:00:00.0,7130,COMPLETE
7,2013-07-25 00:00:00.0,4530,COMPLETE
12,2013-07-25 00:00:00.0,1837,CLOSED
15,2013-07-25 00:00:00.0,2568,COMPLETE
17,2013-07-25 00:00:00.0,2667,COMPLETE
18,2013-07-25 00:00:00.0,1205,CLOSED

// So further operations only have to be done on above dataset

//aim to get KV of (order_id,order_date) 
val ordersMap = ordersFiltered.map( order => (order.split(",")(0).toInt,order.split(",")(1)))

ordersMap.take(10).foreach(println)

(1,2013-07-25 00:00:00.0)
(3,2013-07-25 00:00:00.0)
(4,2013-07-25 00:00:00.0)
(5,2013-07-25 00:00:00.0)
(6,2013-07-25 00:00:00.0)
(7,2013-07-25 00:00:00.0)
(12,2013-07-25 00:00:00.0)
(15,2013-07-25 00:00:00.0)
(17,2013-07-25 00:00:00.0)
(18,2013-07-25 00:00:00.0)


//to convert KV pari as (order_item_order_id, (product_id,order_item_subtotal)) 
val orderItemsMap = orderItems.map( oi => (oi.split(",")(1).toInt,(oi.split(",")(2).toInt, oi.split(",")(4).toFloat)))


orderItemsMap.take(10).foreach(println)
(1,(957,299.98))
(2,(1073,199.99))
(2,(502,250.0))
(2,(403,129.99))
(4,(897,49.98))
(4,(365,299.95))
(4,(502,150.0))
(4,(1014,199.92))
(5,(957,299.98))
(5,(365,299.95))


scala> ordersMap
res5: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[9] at map at <console>:31

scala> orderItemsMap
res9: org.apache.spark.rdd.RDD[(Int, (Int, Float))] = MapPartitionsRDD[11] at map at <console>:29


scala> val ordersJoin = ordersMap.join(orderItemsMap)
ordersJoin: org.apache.spark.rdd.RDD[(Int, (String, (Int, Float)))] = MapPartitionsRDD[17] at join at <console>:37

after joining the output is = (order_id,(order_date,(product_id,order_item_subtotal)))

ordersJoin.take(10).foreach(println)
(65722,(2014-05-23 00:00:00.0,(365,119.98)))
(65722,(2014-05-23 00:00:00.0,(730,400.0)))
(65722,(2014-05-23 00:00:00.0,(1004,399.98)))
(65722,(2014-05-23 00:00:00.0,(627,199.95)))
(65722,(2014-05-23 00:00:00.0,(191,199.98)))
(23776,(2013-12-20 00:00:00.0,(1073,199.99)))
(23776,(2013-12-20 00:00:00.0,(403,129.99)))
(53926,(2014-06-30 00:00:00.0,(365,119.98)))
(53926,(2014-06-30 00:00:00.0,(191,99.99)))
(51620,(2014-06-13 00:00:00.0,(1004,399.98)))

(order_id,(order_date,(product_id,order_item_subtotal)))

// Now since the joining is done , let us look for required output i.e daily revenue

Required output for that. =  ((order_date,product_id),order_item_subtotal)
so convert (order_id,(order_date,(product_id,order_item_subtotal))) => to   ((order_date,product_id),order_item_subtotal)
val ordersJoinMap= ordersJoin.map(rec=>((rec._2._1,rec._2._2._1),rec._2._2._2 ))

((order_date,product_id),order_item_subtotal)

ordersJoinMap.take(10).foreach(println)
((2014-05-23 00:00:00.0,365),119.98)
((2014-05-23 00:00:00.0,730),400.0)
((2014-05-23 00:00:00.0,1004),399.98)
((2014-05-23 00:00:00.0,627),199.95)
((2014-05-23 00:00:00.0,191),199.98)
((2013-12-20 00:00:00.0,1073),199.99)
((2013-12-20 00:00:00.0,403),129.99)
((2014-06-30 00:00:00.0,365),119.98)
((2014-06-30 00:00:00.0,191),99.99)
((2014-06-13 00:00:00.0,1004),399.98)

scala> ordersJoinMap.count
res12: Long = 75408
scala> ordersJoin.count
res13: Long = 75408

scala> val dailyRevenuePerProductId = ordersJoinMap.reduceByKey((revenue,order_item_subtotal)=>revenue+order_item_subtotal)
dailyRevenuePerProductId: org.apache.spark.rdd.RDD[((String, Int), Float)] = ShuffledRDD[19] at reduceByKey at <console>:41

dailyRevenuePerProductId.take(10).foreach(println)
((2014-07-17 00:00:00.0,403),3379.7402)
((2013-11-21 00:00:00.0,982),149.99)
((2013-10-11 00:00:00.0,116),224.95)
((2014-01-06 00:00:00.0,564),60.0)
((2013-11-06 00:00:00.0,885),74.97)
((2014-02-26 00:00:00.0,572),199.95)
((2014-03-05 00:00:00.0,1073),2399.8801)
((2014-03-19 00:00:00.0,565),140.0)
((2014-07-15 00:00:00.0,135),110.0)
((2014-06-24 00:00:00.0,403),2079.84)


// ((order_date,product_id) , daily_revenue)
scala> dailyRevenuePerProductId.count
res17: Long = 9120

so far we have output as ((order_date,product_id) , daily_revenue) 
but the required output is includes product_name as well which is part of products dataset (which is present in local file system)



import scala.io.Source
scala> val productsRaw=Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
productsRaw: List[String] = List(1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy, 2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat, 3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat, 4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat, 5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet, 6,2,Jordan Men's VI Retro TD Football Cleat,,134.99,h...

scala> productsRaw.take(10).foreach(println)
1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy
2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat
3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat
4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat
5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet
6,2,Jordan Men's VI Retro TD Football Cleat,,134.99,http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat
7,2,Schutt Youth Recruit Hybrid Custom Football H,,99.99,http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014
8,2,Nike Men's Vapor Carbon Elite TD Football Cle,,129.99,http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat
9,2,Nike Adult Vapor Jet 3.0 Receiver Gloves,,50.0,http://images.acmesports.sports/Nike+Adult+Vapor+Jet+3.0+Receiver+Gloves
10,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat

now convert the list to RDD :
val products=sc.parallelize(productsRaw)

scala> val productsMap=products.map(p =>(p.split(",")(0).toInt,p.split(",")(2)))
productsMap: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[22] at map at <console>:32

scala> products.count
res20: Long = 1345
scala> productsMap.count
res21: Long = 1345

productsMap.take(10).foreach(println)

(1,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U)
(2,Under Armour Men's Highlight MC Football Clea)
(3,Under Armour Men's Renegade D Mid Football Cl)
(4,Under Armour Men's Renegade D Mid Football Cl)
(5,Riddell Youth Revolution Speed Custom Footbal)
(6,Jordan Men's VI Retro TD Football Cleat)
(7,Schutt Youth Recruit Hybrid Custom Football H)
(8,Nike Men's Vapor Carbon Elite TD Football Cle)
(9,Nike Adult Vapor Jet 3.0 Receiver Gloves)
(10,Under Armour Men's Highlight MC Football Clea)

output now is = (product_id,product_name)



so we need to join productsMap and dailyRevenuePerProductId datasets now
i.e (product_id,product_name)  join with ((order_date,order_item_product_id) , daily_revenue)

But as we can see both of the above datasets doest not have key in common so we need to position the product_id as key in order to join

We need final output as below 
(product_id,(product_name,order_date,daily_revenue))


for that we need to convert the dailyRevenuePerProductId data set structure as below
(order_item_product_id,(order_date, daily_revenue))

scala> val dailyRevenuePerProductId_for_join= dailyRevenuePerProductId.map(rec => (rec._1._2,(rec._1._1,rec._2)))
dailyRevenuePerProductId_for_join: org.apache.spark.rdd.RDD[(Int, (String, Float))] = MapPartitionsRDD[24] at map at <console>:44


scala> dailyRevenuePerProductId_for_join.take(10).foreach(println)

(403,(2014-07-17 00:00:00.0,3379.7402))
(982,(2013-11-21 00:00:00.0,149.99))
(116,(2013-10-11 00:00:00.0,224.95))
(564,(2014-01-06 00:00:00.0,60.0))
(885,(2013-11-06 00:00:00.0,74.97))
(572,(2014-02-26 00:00:00.0,199.95))
(1073,(2014-03-05 00:00:00.0,2399.8801))
(565,(2014-03-19 00:00:00.0,140.0))
(135,(2014-07-15 00:00:00.0,110.0))
(403,(2014-06-24 00:00:00.0,2079.84))



now (order_item_product_id,(order_date, daily_revenue)) joining with  (product_id,product_name)

scala> dailyRevenuePerProductId_for_join
res29: org.apache.spark.rdd.RDD[(Int, (String, Float))] = MapPartitionsRDD[25] at map at <console>:44

scala> productsMap
res30: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[22] at map at <console>:32



scala> val dailyRevenuePerProductJoin= dailyRevenuePerProductId_for_join.join(productsMap)
dailyRevenuePerProductJoin: org.apache.spark.rdd.RDD[(Int, ((String, Float), String))] = MapPartitionsRDD[28] at join at <console>:52

dailyRevenuePerProductJoin.take(10).foreach(println)

(product_id, ( (order_date,daily_revenue),product_name))

(   1      ,2(1(    1     ,        2    ), 2          ))


(24,((2013-11-19,239.97),Elevation Training Mask 2.0))
(24,((2014-04-01,159.98),Elevation Training Mask 2.0))
(24,((2013-12-06,239.97),Elevation Training Mask 2.0))
(24,((2013-08-18,399.95),Elevation Training Mask 2.0))
(24,((2013-09-01,399.95),Elevation Training Mask 2.0))
(24,((2013-08-16,399.95),Elevation Training Mask 2.0))
(24,((2014-04-06,239.97),Elevation Training Mask 2.0))
(24,((2013-10-06,79.99),Elevation Training Mask 2.0))
(24,((2013-11-02,79.99),Elevation Training Mask 2.0))
(24,((2014-02-14,159.98),Elevation Training Mask 2.0))

scala> dailyRevenuePerProductId_for_join.count
res32: Long = 9120
scala> productsMap.count
res33: Long = 1345


order_date,daily_revenue_per_product,product_name  .. But these should be sorted before we present this in output.


so from dataset 'dailyRevenuePerProductJoin' we need to discard unnessary fields. i.e discard product_id

// (order_date ASC + daily_Revenue in DESC order) as Key for soryByKey() here 

convert as following

 (product_id,(order_date,daily_revenue),product_name) =======> ((key),(final output as in tuple as value))
 ======>((order_date_ASC,daily_revenue_DESC),product_name,order_date,daily_revenue)


val dailyRevenuePerProductSorted = dailyRevenuePerProductJoin.map(rec=>((rec._2._1._1,-rec._2._1._2),(rec._2._1._1,rec._2._1._2,rec._2._2))).sortByKey()


scala> dailyRevenuePerProductSorted.take(20).foreach(println)
((2013-07-25 00:00:00.0,-5599.72),(2013-07-25 00:00:00.0,5599.72,Field & Stream Sportsman 16 Gun Fire Safe))
((2013-07-25 00:00:00.0,-5099.49),(2013-07-25 00:00:00.0,5099.49,Nike Men's Free 5.0+ Running Shoe))
((2013-07-25 00:00:00.0,-4499.7),(2013-07-25 00:00:00.0,4499.7,Diamondback Women's Serene Classic Comfort Bi))
((2013-07-25 00:00:00.0,-3359.44),(2013-07-25 00:00:00.0,3359.44,Perfect Fitness Perfect Rip Deck))
((2013-07-25 00:00:00.0,-2999.85),(2013-07-25 00:00:00.0,2999.85,Pelican Sunstream 100 Kayak))
((2013-07-25 00:00:00.0,-2798.88),(2013-07-25 00:00:00.0,2798.88,O'Brien Men's Neoprene Life Vest))
((2013-07-25 00:00:00.0,-1949.8501),(2013-07-25 00:00:00.0,1949.8501,Nike Men's CJ Elite 2 TD Football Cleat))
((2013-07-25 00:00:00.0,-1650.0),(2013-07-25 00:00:00.0,1650.0,Nike Men's Dri-FIT Victory Golf Polo))
((2013-07-25 00:00:00.0,-1079.73),(2013-07-25 00:00:00.0,1079.73,Under Armour Girls' Toddler Spine Surge Runni))
((2013-07-25 00:00:00.0,-599.99),(2013-07-25 00:00:00.0,599.99,Bowflex SelectTech 1090 Dumbbells))
((2013-07-25 00:00:00.0,-319.96),(2013-07-25 00:00:00.0,319.96,Elevation Training Mask 2.0))
((2013-07-25 00:00:00.0,-207.96),(2013-07-25 00:00:00.0,207.96,Titleist Pro V1 High Numbers Personalized Gol))
((2013-07-25 00:00:00.0,-199.99),(2013-07-25 00:00:00.0,199.99,Nike Men's Kobe IX Elite Low Basketball Shoe))
((2013-07-25 00:00:00.0,-119.99),(2013-07-25 00:00:00.0,119.99,Cleveland Golf Women's 588 RTX CB Satin Chrom))
((2013-07-25 00:00:00.0,-119.97),(2013-07-25 00:00:00.0,119.97,TYR Boys' Team Digi Jammer))
((2013-07-25 00:00:00.0,-109.99),(2013-07-25 00:00:00.0,109.99,Merrell Men's All Out Flash Trail Running Sho))
((2013-07-25 00:00:00.0,-108.0),(2013-07-25 00:00:00.0,108.0,LIJA Women's Button Golf Dress))
((2013-07-25 00:00:00.0,-100.0),(2013-07-25 00:00:00.0,100.0,Nike Women's Legend V-Neck T-Shirt))

((key value where we use negation for sorting ..should not be part of output..can be discarded),(value ..which is needed for output ))


 /****  alternative way for special case :
 val dailyRevenuePerProduct= dailyRevenuePerProductSorted.map((rec => rec._2).mkString(",")) // this will work only if all the values or rec are String type.
*****/
but dailyRevenuePerProductSorted's rec._2 is  (String, Float, String) so mkString will not work as it contains float.
res2: org.apache.spark.rdd.RDD[((String, Float), (String, Float, String))] = ShuffledRDD[21] at sortByKey at <console>:54

 val dailyRevenuePerProduct= dailyRevenuePerProductSorted.map((rec => rec._2._1+","+rec._2._2+","+rec._2._3))
res4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[22] at map at <console>:56

scala> dailyRevenuePerProduct.take(10).foreach(println)
2013-07-25 00:00:00.0,5599.72,Field & Stream Sportsman 16 Gun Fire Safe
2013-07-25 00:00:00.0,5099.49,Nike Men's Free 5.0+ Running Shoe
2013-07-25 00:00:00.0,4499.7,Diamondback Women's Serene Classic Comfort Bi
2013-07-25 00:00:00.0,3359.44,Perfect Fitness Perfect Rip Deck
2013-07-25 00:00:00.0,2999.85,Pelican Sunstream 100 Kayak
2013-07-25 00:00:00.0,2798.88,O'Brien Men's Neoprene Life Vest
2013-07-25 00:00:00.0,1949.8501,Nike Men's CJ Elite 2 TD Football Cleat
2013-07-25 00:00:00.0,1650.0,Nike Men's Dri-FIT Victory Golf Polo
2013-07-25 00:00:00.0,1079.73,Under Armour Girls' Toddler Spine Surge Runni
2013-07-25 00:00:00.0,599.99,Bowflex SelectTech 1090 Dumbbells


#Saving the output as textFile in HDFS:
scala> dailyRevenuePerProduct.saveAsTextFile("/user/rajeshs/daily_revenue_text_scala")
# Ways of previewing the saved data

1) From spark Shell itself
// To preview the data in HDFS from SparkShell by using sc.textFile("path").take(10).foreach(println) 

sc.textFile("/user/rajeshs/daily_revenue_text_scala").take(10).foreach(println)
2013-07-25 00:00:00.0,5599.72,Field & Stream Sportsman 16 Gun Fire Safe
2013-07-25 00:00:00.0,5099.49,Nike Men's Free 5.0+ Running Shoe
2013-07-25 00:00:00.0,4499.7,Diamondback Women's Serene Classic Comfort Bi
2013-07-25 00:00:00.0,3359.44,Perfect Fitness Perfect Rip Deck
2013-07-25 00:00:00.0,2999.85,Pelican Sunstream 100 Kayak
2013-07-25 00:00:00.0,2798.88,O'Brien Men's Neoprene Life Vest
2013-07-25 00:00:00.0,1949.8501,Nike Men's CJ Elite 2 TD Football Cleat
2013-07-25 00:00:00.0,1650.0,Nike Men's Dri-FIT Victory Golf Polo
2013-07-25 00:00:00.0,1079.73,Under Armour Girls' Toddler Spine Surge Runni
2013-07-25 00:00:00.0,599.99,Bowflex SelectTech 1090 Dumbbells


2) from HDFS command line :
[rajeshs@gw02 ~]$ hdfs dfs -tail /user/rajeshs/daily_revenue_text_scala/part-00000
0199.32,Diamondback Women's Serene Classic Comfort Bi
2014-01-30 00:00:00.0,6718.8794,Perfect Fitness Perfect Rip Deck
2014-01-30 00:00:00.0,5999.4,Nike Men's Free 5.0+ Running Shoe
2014-01-30 00:00:00.0,5199.6,Nike Men's CJ Elite 2 TD Football Cleat
2014-01-30 00:00:00.0,4399.7803,Pelican Sunstream 100 Kayak
2014-01-30 00:00:00.0,4398.2393,O'Brien Men's Neoprene Life Vest
2014-01-30 00:00:00.0,3550.0,Nike Men's Dri-FIT Victory Golf Polo


3) Web interface : Ambari or Hue 

http://gw01.itversity.com:8080/


Now we have to copy the hdfs directory to local file system  at " /home/rajeshs/daily_revenue_scala "(as part of the Problem Statement)

[rajeshs@gw02 ~]$ hdfs dfs -get /user/rajeshs/daily_revenue_text_scala /home/rajeshs/daily_revenue_scala
[rajeshs@gw02 ~]$ cd daily_revenue_scala/
[rajeshs@gw02 daily_revenue_scala]$ ll
total 596
-rw-r--r-- 1 rajeshs students 318059 May 24 14:02 part-00000
-rw-r--r-- 1 rajeshs students 288307 May 24 14:02 part-00001
-rw-r--r-- 1 rajeshs students      0 May 24 14:02 _SUCCESS

// to delete the directory along with its contents  : rm -rf directory_name
[rajeshs@gw02 ~]$ rm -rf daily_revenue_scala/



scala> :history
dailyRevenuePerProductId_for_join.count
productsMap.count









/****** total code *********/
val orders=sc.textFile("/public/retail_db/orders")
val orderItems = sc.textFile("/public/retail_db/order_items")
val ordersFiltered = orders.filter(o => (o.split(",")(3) == "COMPLETE" ||o.split(",")(3) == "CLOSED"))
val ordersMap = ordersFiltered.map( order => (order.split(",")(0).toInt,order.split(",")(1)))
val orderItemsMap = orderItems.map( oi => (oi.split(",")(1).toInt,(oi.split(",")(2).toInt, oi.split(",")(4).toFloat)))
val ordersJoin = ordersMap.join(orderItemsMap)
val ordersJoinMap= ordersJoin.map(rec=>((rec._2._1,rec._2._2._1),rec._2._2._2 ))
val dailyRevenuePerProductId = ordersJoinMap.reduceByKey((revenue,order_item_subtotal)=>revenue+order_item_subtotal)
import scala.io.Source
val productsRaw=Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
val products=sc.parallelize(productsRaw)
val productsMap=products.map(p =>(p.split(",")(0).toInt,p.split(",")(2)))
productsMap.take(10).foreach(println)
val dailyRevenuePerProductId_for_join= dailyRevenuePerProductId.map(rec => (rec._1._2,(rec._1._1,rec._2)))
val dailyRevenuePerProductJoin= dailyRevenuePerProductId_for_join.join(productsMap)
dailyRevenuePerProductJoin.take(10).foreach(println)


/****** total code *********/






Final notes from resources :

// Read orders and order_items
val orders = sc.textFile("/public/retail_db/orders")
val orderItems = sc.textFile("/public/retail_db/order_items")

orders.first
orderItems.first

orders.take(10).foreach(println)
orderItems.take(10).foreach(println)

// Filter for completed or closed orders
orders.
  map(order => order.split(",")(3)).
  distinct.
  collect.
  foreach(println)
val ordersFiltered = orders.
  filter(order => order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED")
ordersFiltered.take(100).foreach(println)

// Convert both filtered orders and order_items to key value pairs
val ordersMap = ordersFiltered.
  map(order => (order.split(",")(0).toInt, order.split(",")(1)))
val orderItemsMap = orderItems.
  map(oi => (oi.split(",")(1).toInt,(oi.split(",")(2).toInt, oi.split(",")(4).toFloat)))

ordersMap.take(10).foreach(println)
orderItemsMap.take(10).foreach(println)

// Join the two data sets
val ordersJoin = ordersMap.join(orderItemsMap)
ordersJoin.take(10).foreach(println)
ordersJoin.count


/*************** 90. Solution - Get Daily Revenue per Product - Read and join orders and order_items ************end******* /


/*************** 91. Solution - Get Daily Revenue per Product - Compute daily revenue per product id ************Start******* /




Final Notes from resources:
Solution – Get Daily Revenue per Product – Compute daily revenue per product id

//(order_id, (order_date, (order_item_product_id, order_item_subtotal)))
// Get daily revenue per product id
val ordersJoinMap = ordersJoin.map(rec => ((rec._2._1, rec._2._2._1), rec._2._2._2))
ordersJoinMap.take(10).foreach(println)
ordersJoinMap.count
//((order_date, order_item_product_id), order_item_subtotal)
val dailyRevenuePerProductId = ordersJoinMap.
  reduceByKey((revenue, order_item_subtotal) => revenue + order_item_subtotal)
dailyRevenuePerProductId.take(10).foreach(println)
dailyRevenuePerProductId.count
//((order_date, order_item_product_id), daily_revenue_per_product_id)

	/*************** 91. Solution - Get Daily Revenue per Product - Compute daily revenue per product id ************end******* /


	/*************** 92. Solution - Get Daily Revenue per Product - Read products data and create RDD ************Start******* /

// Load products from local file system and convert into RDD /data/retail_db/products/part-00000

import scala.io.Source
val productsRaw = Source.
  fromFile("/data/retail_db/products/part-00000").
  getLines.
  toList
val products = sc.parallelize(productsRaw)
products.take(10).foreach(println)
products.count

// Join daily revenue per product id with products to get daily revenue per product (by name)
val productsMap = products.
  map(product => (product.split(",")(0).toInt, product.split(",")(2)))
productsMap.take(10).foreach(println)
productsMap.count

//((order_date, order_product_id), daily_revenue_per_product_id)
val dailyRevenuePerProductIdMap = dailyRevenuePerProductId.
  map(rec => (rec._1._2, (rec._1._1, rec._2)))
dailyRevenuePerProductIdMap.take(10).foreach(println)
//(order_product_id, (order_date, daily_revenue_per_product_id))

val dailyRevenuePerProductJoin = dailyRevenuePerProductIdMap.join(productsMap)
//(order_product_id, ((order_date, daily_revenue_per_product_id), product_name))


	/*************** 92. Solution - Get Daily Revenue per Product - Read products data and create RDD ************end******* /



	/************************************93. Solution - Get Daily Revenue per Product - Sort and save to HDFS ***********Start*************/




// Sort the data by date in ascending order and by daily revenue per product in descending order
val dailyRevenuePerProductSorted = dailyRevenuePerProductJoin.
  map(rec => ((rec._2._1._1, -rec._2._1._2), (rec._2._1._1, rec._2._1._2, rec._2._2))).
  sortByKey()
dailyRevenuePerProductSorted.take(100).foreach(println)
//((order_date_asc, daily_revenue_per_product_id_desc), (order_date,daily_revenue_per_product,product_name))

// Get data to desired format – order_date,daily_revenue_per_product,product_name
val dailyRevenuePerProduct = dailyRevenuePerProductSorted.
  map(rec => rec._2._1 + "," + rec._2._2 + "," + rec._2._3)
dailyRevenuePerProduct.take(10).foreach(println)

// Save final output into HDFS in avro file format as well as text file format
// HDFS location – avro format /user/YOUR_USER_ID/daily_revenue_avro_scala
// HDFS location – text format /user/YOUR_USER_ID/daily_revenue_txt_scala
dailyRevenuePerProduct.saveAsTextFile("/user/dgadiraju/daily_revenue_txt_scala")
sc.textFile("/user/dgadiraju/daily_revenue_txt_scala").take(10).foreach(println)
// Copy both from HDFS to local file system
// /home/YOUR_USER_ID/daily_revenue_scala
// mkdir daily_revenue_scala
// hadoop fs -get /user/dgadiraju/daily_revenue_txt_scala \
// /home/dgadiraju/daily_revenue_scala/daily_revenue_txt_scala
// cd daily_revenue_scala/daily_revenue_txt_scala/
// ls -ltr


dailyRevenuePerProduct.saveAsTextFile("/user/rajeshs/daily_revenue_txt_scala_latest")

	/************************************93. Solution - Get Daily Revenue per Product - Sort and save to HDFS ***********Start*************/

	